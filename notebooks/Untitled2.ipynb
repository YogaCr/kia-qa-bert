{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb64dfea",
   "metadata": {},
   "source": [
    "Pakai PyTorch :\n",
    "- Pakai bantuan MPS kena out of memory\n",
    "- Pakai pure CPU bisa sampai 100 jam\n",
    "\n",
    "Next coba pakai versi TF? \n",
    "https://huggingface.co/docs/transformers/tasks/question_answering#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7121e57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"indolem/indobert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44e7ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dev_data = json.load(open('./dev-v2.0.json'))\n",
    "train_data = json.load(open('./train-v2.0.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b9f73f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69fb7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9efa22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = []\n",
    "train_questions = []\n",
    "train_answers = []\n",
    "\n",
    "train_df_datas = []\n",
    "for data in train_data['data']:\n",
    "    for paragraph in data['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        for qa in paragraph['qas']:\n",
    "            question = qa['question']\n",
    "            for answer in qa['answers']:\n",
    "                train_texts.append(context)\n",
    "                train_questions.append(question)\n",
    "                train_answers.append(answer)\n",
    "                train_dict = {\n",
    "                    'context':context,'question':question,'answer':answer\n",
    "                }\n",
    "                train_df_datas.append(train_dict)\n",
    "train_df = pd.DataFrame(train_df_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ec778a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86821, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cbdacc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05ff4f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts = []\n",
    "val_questions = []\n",
    "val_answers = []\n",
    "\n",
    "val_df_datas=[]\n",
    "\n",
    "for data in dev_data['data']:\n",
    "    for paragraph in data['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        for qa in paragraph['qas']:\n",
    "            question = qa['question']\n",
    "            for answer in qa['answers']:\n",
    "                val_texts.append(context)\n",
    "                val_questions.append(question)\n",
    "                val_answers.append(answer)\n",
    "                val_dict = {\n",
    "                    'context':context,'question':question,'answer':answer\n",
    "                }\n",
    "                val_df_datas.append(val_dict)\n",
    "val_df = pd.DataFrame(val_df_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d7d378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20302, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dae3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = val_df.iloc[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3b85467",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "stride = 128\n",
    "\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answer\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"]\n",
    "        end_char = answer[\"answer_start\"] + len(answer[\"text\"])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc1a1b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.Dataset.from_pandas(train_df)\n",
    "val_dataset = datasets.Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d668d95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.86ba/s]\n"
     ]
    }
   ],
   "source": [
    "train_set = train_dataset.map(\n",
    "    preprocess_data,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bac495",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14cb53c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.09ba/s]\n"
     ]
    }
   ],
   "source": [
    "val_set = val_dataset.map(\n",
    "    preprocess_data,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512d1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c921c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1,2,3], device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ff815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3466ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"init-train\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9ca505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b318d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b362c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTORCH_MPS_HIGH_WATERMARK_RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67db39ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84511629",
   "metadata": {},
   "source": [
    "Coba pakai TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fb865",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b382fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "total_train_steps = (len(train_set) // batch_size) * num_epochs\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=total_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e79b5009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"indolem/indobert-base-uncased\",from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "278d98bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    train_set,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    val_set,\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60d5559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d437cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "581875a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "505630ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/miniforge3/envs/bert_env/lib/python3.8/site-packages/huggingface_hub-0.12.0-py3.8.egg/huggingface_hub/utils/_deprecation.py:128: FutureWarning: 'set_access_token' (from 'huggingface_hub.hf_api') is deprecated and will be removed from version '0.14'. `HfApi.set_access_token` is deprecated as it is very ambiguous. Use `login` or `set_git_credential` instead.\n",
      "/Users/macbook/miniforge3/envs/bert_env/lib/python3.8/site-packages/huggingface_hub-0.12.0-py3.8.egg/huggingface_hub/utils/_deprecation.py:128: FutureWarning: 'write_to_credential_store' (from 'huggingface_hub.utils._git_credential') is deprecated and will be removed from version '0.14'. Please use `huggingface_hub.set_git_credential` instead as it allows the user to chose which git-credential tool to use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "token = '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'\n",
    "\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "# set api for login and save token\n",
    "api=HfApi()\n",
    "api.set_access_token(token)\n",
    "folder = HfFolder()\n",
    "folder.save_token(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29f69e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/Personal Folder/Skripsi Gimang/qa-system/init-train is already a clone of https://huggingface.co/YogaCr/init-train. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "callback = PushToHubCallback(\n",
    "    output_dir=\"init-train\",\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2c285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 68/625 [==>...........................] - ETA: 6:24:43 - loss: 4.3801"
     ]
    }
   ],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef283d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b66e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()\n",
    "with tf.device('/GPU'):\n",
    "  a = tf.random.normal(shape=(2,), dtype=tf.float32)\n",
    "  b = tf.nn.relu(a)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bert_env] *",
   "language": "python",
   "name": "conda-env-bert_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
